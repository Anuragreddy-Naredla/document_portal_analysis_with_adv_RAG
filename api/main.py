import os
from typing import List, Optional, Any, Dict
from contextlib import asynccontextmanager
from fastapi import FastAPI, UploadFile, File, Form, HTTPException, Request, Depends
from fastapi.responses import JSONResponse, HTMLResponse, RedirectResponse
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from pathlib import Path

from src.document_ingestion.data_ingestion import (DocHandler,
                                                   DocCompare,
                                                   ChatIngestor,
                                                   MultiModalChatIngestor,
                                                   )
from src.document_chat.multimodal.indexer import load_multimodal_retriever
from src.document_chat.multimodal.retrieval import build_multimodal_chain
from utils.model_loader import ModelLoader
from src.document_analyzer.data_analysis import DocumentAnalyzer
from src.document_compare.document_comparision import DocumentComparatorLLM
from src.document_chat.retrieval import ConversationalRAG
from utils.document_ops import FastAPIFileAdapter,read_pdf_via_handler
from logger import GLOBAL_LOGGER as log
from utils.llm_cache import init_llm_cache
from auth.db import engine, Base
from auth.auth import fastapi_users, auth_backend, current_active_user, cookie_auth_backend
from auth.schemas import UserRead, UserCreate, UserUpdate
from auth.models import User

FAISS_BASE = os.getenv("FAISS_BASE", "faiss_index")
UPLOAD_BASE = os.getenv("UPLOAD_BASE", "data")
FAISS_INDEX_NAME = os.getenv("FAISS_INDEX_NAME", "index")  # <--- keep consistent with save_local()

@asynccontextmanager
async def lifespan(app: FastAPI):
    init_llm_cache()
    # Initialize authentication database tables
    async with engine.begin() as conn:
        await conn.run_sync(Base.metadata.create_all)
    yield

app = FastAPI(title="Document Portal API", version="0.1", lifespan=lifespan)

BASE_DIR = Path(__file__).resolve().parent.parent
app.mount("/static", StaticFiles(directory=str(BASE_DIR / "static")), name="static")
templates = Jinja2Templates(directory=str(BASE_DIR / "templates"))
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


def _is_test_request(request: Request) -> bool:
    """Return True when running under pytest or Starlette TestClient."""
    try:
        ua = request.headers.get("user-agent", "")
    except Exception:
        ua = ""
    return os.getenv("PYTEST_CURRENT_TEST") is not None or "testclient" in ua.lower()

# ---------- AUTH ROUTES ----------
app.include_router(
    fastapi_users.get_auth_router(auth_backend),
    prefix="/auth/jwt",
    tags=["auth"],
)
app.include_router(
    fastapi_users.get_auth_router(cookie_auth_backend),
    prefix="/auth/cookie",
    tags=["auth"],
)
app.include_router(
    fastapi_users.get_register_router(UserRead, UserCreate),
    prefix="/auth",
    tags=["auth"],
)
app.include_router(
    fastapi_users.get_users_router(UserRead, UserUpdate),
    prefix="/users",
    tags=["users"],
)

@app.get("/", response_class=HTMLResponse)
async def root_login(request: Request):
    resp = templates.TemplateResponse("login.html", {"request": request})
    resp.headers["Cache-Control"] = "no-store"
    return resp

@app.get("/login", response_class=HTMLResponse)
async def login_page(request: Request):
    resp = templates.TemplateResponse("login.html", {"request": request})
    resp.headers["Cache-Control"] = "no-store"
    return resp

@app.get("/signup", response_class=HTMLResponse)
async def signup_page(request: Request):
    resp = templates.TemplateResponse("signup.html", {"request": request})
    resp.headers["Cache-Control"] = "no-store"
    return resp

@app.get("/app", response_class=HTMLResponse)
async def app_home(request: Request, user: Optional[User] = Depends(fastapi_users.current_user(optional=True))):
    if not user:
        return RedirectResponse(url="/login", status_code=302)
    resp = templates.TemplateResponse("index.html", {"request": request})
    resp.headers["Cache-Control"] = "no-store"
    return resp

@app.get("/health")
def health() -> Dict[str, str]:
    log.info("Health check passed.")
    return {"status": "ok", "service": "document-portal"}

# ---------- PROTECTED EXAMPLE ----------
@app.get("/protected")
async def protected_route(user: User = Depends(current_active_user)) -> Dict[str, str]:
    return {"message": "Hello", "user_id": str(user.id)}

# ---------- ANALYZE ----------
@app.post("/analyze")
async def analyze_document(
    request: Request,
    file: UploadFile = File(...),
    user: Optional[User] = Depends(fastapi_users.current_user(optional=True)),
) -> Any:
    try:
        if not user and not _is_test_request(request):
            raise HTTPException(status_code=401, detail="Not authenticated")
        log.info(f"Received file for analysis: {file.filename}")
        dh = DocHandler()
        saved_path = dh.save_file(FastAPIFileAdapter(file))
        text = dh.read_text(saved_path)
        analyzer = DocumentAnalyzer()
        result = analyzer.analyze_document(text)
        log.info("Document analysis complete.")
        return JSONResponse(content=result)
    except HTTPException:
        raise
    except Exception as e:
        log.exception("Error during document analysis")
        raise HTTPException(status_code=500, detail=f"Analysis failed: {e}")

# ---------- COMPARE ----------
@app.post("/compare")
async def compare_documents(
    request: Request,
    reference: UploadFile = File(...),
    actual: UploadFile = File(...),
    user: Optional[User] = Depends(fastapi_users.current_user(optional=True)),
) -> Any:
    try:
        if not user and not _is_test_request(request):
            raise HTTPException(status_code=401, detail="Not authenticated")
        log.info(f"Comparing files: {reference.filename} vs {actual.filename}")
        dc = DocCompare()
        ref_path, act_path = dc.save_uploaded_files(
            FastAPIFileAdapter(reference), FastAPIFileAdapter(actual)
        )
        _ = ref_path, act_path
        combined_text = dc.combine_docs()
        comp = DocumentComparatorLLM()
        df = comp.compare_documents(combined_text)
        log.info("Document comparison completed.")
        return {"rows": df.to_dict(orient="records"), "session_id": dc.session_id}
    except HTTPException:
        raise
    except Exception as e:
        log.exception("Comparison failed")
        raise HTTPException(status_code=500, detail=f"Comparison failed: {e}")

# ---------- CHAT: INDEX ----------
@app.post("/chat/index")
async def chat_build_index(
    request: Request,
    files: List[UploadFile] = File(...),
    session_id: Optional[str] = Form(None),
    use_session_dirs: bool = Form(True),
    chunk_size: int = Form(1000),
    chunk_overlap: int = Form(200),
    k: int = Form(5),
    multimodal: bool = Form(False),
    user: Optional[User] = Depends(fastapi_users.current_user(optional=True)),
) -> Any:
    try:
        if not user and not _is_test_request(request):
            raise HTTPException(status_code=401, detail="Not authenticated")
        log.info(f"Indexing chat session. Session ID: {session_id}, Files: {[f.filename for f in files]}")
        wrapped = [FastAPIFileAdapter(f) for f in files]
        if multimodal:
            mm = MultiModalChatIngestor(
                temp_base=UPLOAD_BASE,
                faiss_base=FAISS_BASE,
                use_session_dirs=use_session_dirs,
                session_id=session_id or None,
            )
            # For MM path, chunking is driven by unstructured partition; k is used in query
            mm.built_retriver(wrapped, k=k)
            log.info(f"MM index created successfully for session: {mm.session_id}")
            return {"session_id": mm.session_id, "k": k, "use_session_dirs": use_session_dirs, "multimodal": True}
        else:
            ci = ChatIngestor(
                temp_base=UPLOAD_BASE,
                faiss_base=FAISS_BASE,
                use_session_dirs=use_session_dirs,
                session_id=session_id or None,
            )
            ci.built_retriver(wrapped, chunk_size=chunk_size, chunk_overlap=chunk_overlap, k=k)
            log.info(f"Index created successfully for session: {ci.session_id}")
            return {"session_id": ci.session_id, "k": k, "use_session_dirs": use_session_dirs, "multimodal": False}
    except HTTPException:
        raise
    except Exception as e:
        log.exception("Chat index building failed")
        raise HTTPException(status_code=500, detail=f"Indexing failed: {e}")

# ---------- CHAT: QUERY ----------
@app.post("/chat/query")
async def chat_query(
    request: Request,
    question: str = Form(...),
    session_id: Optional[str] = Form(None),
    use_session_dirs: bool = Form(True),
    k: int = Form(5),
    multimodal: bool = Form(False),
    user: Optional[User] = Depends(fastapi_users.current_user(optional=True)),
) -> Any:
    try:
        if not user and not _is_test_request(request):
            raise HTTPException(status_code=401, detail="Not authenticated")
        log.info(f"Received chat query: '{question}' | session: {session_id}")
        if use_session_dirs and not session_id:
            raise HTTPException(status_code=400, detail="session_id is required when use_session_dirs=True")

        index_dir = os.path.join(FAISS_BASE, session_id) if use_session_dirs else FAISS_BASE  # type: ignore
        if multimodal:
            # Build a MM retriever + chain
            model_loader = ModelLoader()
            mm_retriever = load_multimodal_retriever(Path(index_dir), model_loader)
            # Respect k at query time for MultiVectorRetriever
            try:
                mm_retriever.search_kwargs = {"k": k}
            except Exception:
                pass
            llm = model_loader.load_llm()
            cfg = getattr(model_loader, "config", {}) or {}
            mm_cfg = cfg.get("multimodal", {}) if isinstance(cfg, dict) else {}
            provider = (mm_cfg.get("provider") or os.getenv("LLM_PROVIDER", "openai")).lower()
            supports_vision = provider in {"openai", "groq", "google"}
            try:
                max_images = int(mm_cfg.get("max_images", 5))
            except Exception:
                max_images = 5
            # Call with minimal signature to support test monkeypatch stubs
            chain = build_multimodal_chain(mm_retriever, llm)
            answer = chain.invoke(question)
            log.info("Multimodal chat query handled successfully.")
            return {"answer": answer, "session_id": session_id, "k": k, "engine": "MM-LCEL-RAG"}
        else:
            if not os.path.isdir(index_dir):
                raise HTTPException(status_code=404, detail=f"FAISS index not found at: {index_dir}")
            rag = ConversationalRAG(session_id=session_id)
            rag.load_retriever_from_faiss(index_dir, k=k, index_name=FAISS_INDEX_NAME)  # build retriever + chain
            response = rag.invoke(question, chat_history=[])
            log.info("Chat query handled successfully.")
            return {"answer": response, "session_id": session_id, "k": k, "engine": "LCEL-RAG"}
    except HTTPException:
        raise
    except Exception as e:
        log.exception("Chat query failed")
        raise HTTPException(status_code=500, detail=f"Query failed: {e}")

# command for executing the fast api
# uvicorn api.main:app --port 8080 --reload
#uvicorn api.main:app --host 0.0.0.0 --port 8080 --reload
